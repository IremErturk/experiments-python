{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5f5655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/28 16:22:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/28 16:22:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('partitions') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a0de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555ff9a",
   "metadata": {},
   "source": [
    "### Step 0: Prepare Sample Data:\n",
    "\n",
    "Pull Green Taxi data in 2019-2020 \n",
    "The csv files will be stored in /data/raw/green folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f736a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83658bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = types.StructType([\n",
    "    types.StructField(\"VendorID\", types.IntegerType(), True),\n",
    "    types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n",
    "    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"passenger_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_distance\", types.DoubleType(), True),\n",
    "    types.StructField(\"fare_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"extra\", types.DoubleType(), True),\n",
    "    types.StructField(\"mta_tax\", types.DoubleType(), True),\n",
    "    types.StructField(\"tip_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"ehail_fee\", types.DoubleType(), True),\n",
    "    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n",
    "    types.StructField(\"total_amount\", types.DoubleType(), True),\n",
    "    types.StructField(\"payment_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"trip_type\", types.IntegerType(), True),\n",
    "    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n",
    "])\n",
    "\n",
    "YEARS = [2019, 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95965778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_data(taxi_color:str ,schema: types.StructType):\n",
    "    input_paths = []\n",
    "    for year in YEARS:\n",
    "        for month in range(1, 13):\n",
    "            input_paths.append(f'data/raw/{taxi_color}/{year}/{month:02d}/')\n",
    "    \n",
    "    print(f'processing data for {taxi_color}_tripdata')    \n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(input_paths)\n",
    "    print(f'data processing finished for {taxi_color}_tripdata')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01256656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for green_tripdata\n",
      "data processing finished for green_tripdata\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, lpep_pickup_datetime: timestamp, lpep_dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: double, improvement_surcharge: double, total_amount: double, payment_type: int, trip_type: int, congestion_surcharge: double]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "green_df = populate_data(\"green\",green_schema)\n",
    "\n",
    "green_df \\\n",
    "    .filter(year(\"lpep_pickup_datetime\") < 2019)\\\n",
    "    .filter(year(\"lpep_pickup_datetime\") > 2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2bfa7670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Partitions 15\n"
     ]
    }
   ],
   "source": [
    "# Default partitions ..\n",
    "print(f'Green Partitions {green_df.rdd.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 : Take a look how the data is written without specific partition configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed999e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "green_df.write.mode(\"overwrite\").csv(\"data/partitions/default.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e634c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      15\r\n"
     ]
    }
   ],
   "source": [
    "! find data/partitions/default.csv ! -name \".*\" ! -name \"_SUCCESS\" -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 : Change Partition count by the coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a446a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "df_coalesce_24 = green_df.coalesce(24)\n",
    "print(df_coalesce_24.rdd.getNumPartitions())\n",
    "# df_coalesce_24.write.mode(\"overwrite\").csv(\"data/partitions/coalesce_24.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9cd75e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_coalesce_8 = green_df.coalesce(8)\n",
    "print(df_coalesce_8.rdd.getNumPartitions())\n",
    "df_coalesce_8.write.mode(\"overwrite\").csv(\"data/partitions/coalesce8.csv\", header=True)\n",
    "\n",
    "## Does not shuffle data therefore, expect to have some size differences in partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3 : Change Partition by repartition: Nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fbbb9810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartition_10 = green_df.repartition(10)\n",
    "print(df_repartition_10.rdd.getNumPartitions())\n",
    "df_repartition_10.write.mode(\"overwrite\").csv(\"data/partitions/repartition10.csv\", header=True)\n",
    "\n",
    "## Does shuffle data therefore, expect to have similiar size partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4 : Change Partition by repartition: Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "55ff70b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartition_payment_type = green_df.repartition(\"payment_type\")\n",
    "print(df_repartition_payment_type.rdd.getNumPartitions())\n",
    "df_repartition_payment_type.write.mode(\"overwrite\").csv(\"data/partitions/repartition_payment_type.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 5 : Change Partition by repartition: Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cfa87ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "green_df = green_df \\\n",
    "    .withColumn(\"pickup_year\", year(\"lpep_pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_month\", month(\"lpep_pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_day\", dayofmonth(\"lpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0d0263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartition_year_month_day = green_df.repartition(\"pickup_year\", \"pickup_month\", \"pickup_day\")\n",
    "print(df_repartition_year_month_day.rdd.getNumPartitions())\n",
    "df_repartition_year_month_day.write.mode(\"overwrite\").csv(\"data/partitions/repartition_year_month_day.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb142dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f00eeaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "green_df.write.partitionBy(\"pickup_year\", \"pickup_month\", \"pickup_day\").mode(\"overwrite\").csv(\"data/partitions/partitionBy_year_month_day.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "febcc273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartion_partionBy_pickupYear = green_df \\\n",
    "                                      .repartition(\"pickup_year\") \\\n",
    "                                      .write \\\n",
    "                                      .partitionBy(\"pickup_year\") \\\n",
    "                                      .mode(\"overwrite\")\\\n",
    "                                      .csv(\"data/partitions/df_repartion_partionBy_year.csv\", header=True)\n",
    "# green_df.write.mode(\"overwrite\").csv(\"data/partitions/df_repartion_partionBy_pickupYear.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedde4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 8 ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8791b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repartion_3_partionBy_pickupYear = green_df \\\n",
    "                                      .repartition(13) \\\n",
    "                                      .write \\\n",
    "                                      .partitionBy(\"pickup_year\") \\\n",
    "                                      .mode(\"overwrite\")\\\n",
    "                                      .csv(\"data/partitions/df_repartion_3_partionBy_pickupYear.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
